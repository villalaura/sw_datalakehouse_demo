# Ingestion Pipeline (Bronze Layer)
The Bronze Layer is where the raw data is ingested, as it comes from the data source, with the same shape, form and format. The Data in Bronze layer is immutable by definition - as it should be not editable - and stored forever: data in this layer is never deleted. However, some portions of data might be archived. Having data in the native format offers several advantages: in case of need, silver and gold layer can be regenerated by replying data into the system, having original data helps data engineers to create new transformation if new requirements are needed or to ensure data lineage and data accuracy.

## Create ingestion_pipeline in Azure Data Factory
This pipeline performs Data Ingestion. Data would be ingested directly from two different sources:
1. AdventureWorks Database
2. ...

### Setting up linked servers
1. Enter Azure Data Factory Studio at the address https://adf.azure.com/ 
2. On the left menù, choose `Manage` and then select `+` > `New` to create a new linked service
3. Choose `Azure SQL Database`
4. Name the new linked service `ls_sql_adventureworks`. Then select the subscription, the server name and the `AdventureWorks` database. In Authentication Type select `SQL authentication` and input User name and password.
5. Then click `Create` to confirm the new linked service.
6. Again, select `+` > `New` to create a new linked service
7. This time, choose `Azure Data Lake Storage Gen2`
8. Name the new linked service `ls_DataLakehouse`. Then select the subscription and the storage account `sadatalakehousedemo`. In Authentication Type select `SQL authentication` and input User name and password.
9. Then click `Create` to confirm the new linked service.
10. You should have these linked services:
    | Name | Type |
    |----------------|----------------|
    | ls_DataLakehouse | Azure Data Lake Storage Gen2 |
    | ls_sql_adventureworks | Azure SQL Database |

### Adventure works ingestion activities

1. On the left menù, choose `Author` and then select `+` > `Pipeline` > `Pipeline` to create a new Pipeline
2. Under `Properties` > `General` set the `Name` property to `advworks_ingestion_pipeline`
3. Under `Activities` search `Lookup` and move the activities in the right panel to add a new Lookup Activity
4. Rename the Lookup Activity `Get All DB Tables`
5. Under the `Settings` tab, create a new dataset of type `Azure SQL Database`
6. In the `Name` properties set the name of the dataset as `ds_adventureworks` and select `ls_sql_adventureworks` as linked service, then click ok to create the new dataset
7. Uncheck `First row only`
8. Select `Query` in `Use query` and type the following sql code:
    ```sql
    SELECT QUOTENAME(table_schema)+'.'+QUOTENAME(table_name) AS TableName FROM 
    information_Schema.tables WHERE table_name not in 
    ('watermarktable','buildversion','errorlog','database_firewall_rules','ipv6_database_firewall_rules',
    'vGetAllCategories','vProductAndDescription','vProductModelCatalogDescription')
    ```
9. Under `Activities` search `ForEach` and move the activities in the right panel to add a new ForEach Activity
10.  Rename the For Each Activity `For Each Table Ingest Data`
11. Under Settings, type the following code in the field Items:
    ```code
    @activity('Get All DB Tables').output.value
    ```
12. Click on the pen icon to modify the activities inside the ForEach Activity
13. Place a Lookup Activity named `Lookup old watermark`
14. In `Settings` panel select `ds_adventureworks` and keep `First row only` selected
15. Select `Query` and type the following sql code:
    ```sql
    SELECT * FROM [dbo].[WatermarkTable] AS WatermarkValue WHERE TableName = '@{item().TableName}'
    ```
16. Place a Lookup Activity named `Lookup new watermark`
17. In `Settings` panel select `ds_adventureworks` and keep `First row only` selected
18. Select `Query` and type the following sql code:
    ```sql
    SELECT MAX(ModifiedDate) as NewWatermarkvalue FROM @{item().TableName}
    ```
19. Create a new `Copy data` activity named `Copy data from Table`
20. In `Settings` panel select `ds_adventureworks`
21. Select `Query` and type the following sql code:
    ```sql
    SELECT * FROM @{item().TableName} WHERE ModifiedDate > '@{activity('Lookup old Watermark').output.firstRow.WatermarkValue}'
    ```
22. In `Sink` panel create a new dataset of type Azure Data Lake Storage Gen2, format type DelimitedText
23. Name the dataset `ds_datalakehouse` and select `ls_DataLakehouse` as Linked service, click Ok
24. Once created, click on `Open` next to the Dataset name to open the properties. 
25. In `Connection` tab, in `FilePath` set the three textboxes in this way:
    - bronze (text)
    - this code:
        ```sql
        sales/@{activity('Lookup old Watermark').output.firstRow.TableName}/@{formatDateTime(utcnow(),'yyyy')}/@{formatDateTime(utcnow(),'MM')}/@{formatDateTime(utcnow(),'dd')}/@{formatDateTime(utcnow(),'HH')}
        ```
    - this code:
        ```sql
        @concat(dataset().FileTime,'.',dataset().FileExt)
        ```
26. In `Parameters` tab add two parameters:
    | Parameter name | Parameter Type | Default Value |
    |----------------|----------------|---------------|
    | FileTime       | String         | `@pipeline().TriggerTime`|
    | FileExt        | String         | `csv`|
27. Connect the two lookup action to the Copy Data Action. The ForEach activity should look like this:
    ![Ingestion copytable](https://github.com/villalaura/sw_datalakehouse_demo/raw/main/media/ingestion_pipeline_copytable.PNG)
28. Add a new Stored Procedure action named `Update Watermark` and connect the previous activity to the new one
29. In `Settings` select `ls_sql_adventureworks` as linked service and `[dbo].[uspWriteWatermark]` as the store procedure name
30. Click on `Import` in the `Stored procedure parameters` section. This should add two parameters. Configure the parameters as follows:
    | Parameter name | Parameter Type | Default Value |
    |----------------|----------------|---------------|
    | LastModifiedtime       | Datetime         | `@{activity('Lookup new Watermark').output.firstRow.NewWatermarkvalue}`|
    | TableName        | String         | `@{activity('Lookup old Watermark').output.firstRow.TableName}`|
31. Go back to the main Pipeline. The main Activities of the Ingestion Pipeline should look like this:
    ![Ingestion copytable](https://github.com/villalaura/sw_datalakehouse_demo/raw/main/media/ingestion_pipeline_main.PNG)
32. Publish all the changes and test the pipeline.

### Checking bronze data
1. Open the Data Lake and navigate to the Bronze container.
2. You should find the folder `sales`
    ![Ingestion check](https://github.com/villalaura/sw_datalakehouse_demo/raw/main/media/ingestion_check_1.PNG)
3. Inside  folder `sales`, you should have a folder for each AdventureWorks table:
    ![Ingestion check](https://github.com/villalaura/sw_datalakehouse_demo/raw/main/media/ingestion_check_2.PNG)
4. Choose folder `Product` and then navigate at the csv level. You should find a file csv with the ingested data for that table:
    ![Ingestion check](https://github.com/villalaura/sw_datalakehouse_demo/raw/main/media/ingestion_check_3.PNG)

## Updating Database
Next, we'll check if the pipeline works also with database updates.
1. Run the query on the SQL Database Adventure Works:
    ```sql
        -- ORDER HEADER
        INSERT INTO [SalesLT].[SalesOrderHeader]
            ([SalesOrderID],[RevisionNumber],[OrderDate],[DueDate],[ShipDate],[Status],
            [OnlineOrderFlag],[PurchaseOrderNumber],[AccountNumber],
            [CustomerID],[ShipToAddressID],[BillToAddressID],[ShipMethod],
            [CreditCardApprovalCode],[SubTotal],[TaxAmt],[Freight],
            [Comment],[rowguid],[ModifiedDate])
        VALUES
            (71947, 2, GETDATE(), DATEADD(day,10,GETDATE()), DATEADD(day,3,GETDATE()), 5,0, 
            'PO8961171947', '10-4020-000268',29612,
		    649,649,'CARGO TRANSPORT 5',NULL,76.20, 8.10, 7.30, NULL, 
		    'E4138C8F-6D70-43E7-83D6-AE0C3CAEA0D6', GETDATE())
		
		-- ORDER DETAIL    
		   INSERT INTO [SalesLT].[SalesOrderDetail]
           ([SalesOrderID],[OrderQty],[ProductID],[UnitPrice]
           ,[UnitPriceDiscount],[rowguid],[ModifiedDate])
     VALUES
           (71947,2,865,38.10,0,'DF78E756-056B-469A-B7F3-8F68D6489A92',GETDATE())
           
    ```
2. Run the pipeline again. Notice that under the Bronze layer, sales folder, a new folder is created under each table.
For [SalesLT].[SalesOrderDetail] and [SalesLT].[SalesOrderHeader] the folders contain two .csv with the modified data.

3. Update some customer data, running this query on the SQL Database Adventure Works:
    ```sql
    UPDATE [SalesLT].[Customer]
    SET CompanyName = 'ACME inc.', EmailAddress = 'richard.byham@acme.com', ModifiedDate = GETDATE()
    WHERE CustomerID = 29612
    ```
4. Run the pipeline again. Notice that under the Bronze layer, sales folder, a new folder is created under each table.
For [SalesLT].[SalesCustomer] the folder contain a .csv with the modified data.
